 Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.

Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.

Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.

The key elements of deep-learning are artificial neural networks, backpropagation, and gradient descent. Artificial neural networks are composed of layers of neurons, which are simple processing units. Each neuron receives a set of inputs, and sends outputs to other neurons. The neurons in each layer are connected to the neurons in the next and previous layers, forming a feedback loop. The key to deep learning is the use of multiple layers of neurons, which allows the network to learn and represent more complex patterns in the data.

Backpropagation is an algorithm used to train artificial neural networks. It is a form of supervised learning, where the network is presented with a set of input-output pairs, and the goal is to adjust the weights and biases of the network so that it can make accurate predictions on new, unseen data.

Gradient descent is a method used to optimize the performance of the network by adjusting the weights and biases in the direction of the steepest descent of the loss function. This process is repeated many times, until the network converges to a set of weights and biases that result in accurate predictions.

De